{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sunnysavita10/Generative-AI-Indepth-Basic-to-Advance/blob/main/Langchain_memory_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "model=HuggingFacePipeline.from_model_id(\n",
    "                                  model_id=\"meta-llama/Llama-3.2-1B\",\n",
    "                                  task=\"text-generation\",\n",
    "                                  pipeline_kwargs=dict(\n",
    "                                    max_new_tokens=512,\n",
    "                                    do_sample=True,\n",
    "                                    repetition_penalty=1.03,\n",
    "                                    return_full_text=False,\n",
    "                                      ),\n",
    "                                    device=0\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ConversationBufferMemory**\n",
    "\n",
    "The Conversation Buffer Memory in LangChain is a type of memory module that stores the complete conversation history as a buffer (a simple list of messages) and passes it to the language model with each new interaction. This memory type helps the model maintain context over the entire conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUQPzgTMpnVg",
    "outputId": "bfb0c6df-6a4c-49fd-fa49-6c8327b20e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"hi\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmitzKzGoo6e",
    "outputId": "7c8307a1-70f1-450d-e61e-ca7e1be332f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am well, thank you for asking. I am a large language model, trained by Google. I do not have personal experiences or feelings, but I am designed to be informative and helpful. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"hi, how are you please tell me?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3zgleD2norLX"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iK0mnJnho1WH"
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "h2px3_AAo5wX"
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_nviz7-o-Lg",
    "outputId": "e9f3fa5c-f755-4f11-e78f-cd51b4a5774f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qtz58I3fpIRv"
   },
   "outputs": [],
   "source": [
    "memory2 = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kIYkuLPspQx_"
   },
   "outputs": [],
   "source": [
    "memory2.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BweTLFgYpUVR",
    "outputId": "bbca8008-2435-4213-b274-80af5f360b7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content=\"What's up\")]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MATKudmrQRdl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "D8mXubm9pZXp"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "22KrSvL-peAo"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ie3Onspvp6cP"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=model,verbose=True,memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "FtXYKCiQqJSX",
    "outputId": "2546e8c6-aa8d-4444-8b58-b1e8789e85e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Hello! It's nice to meet you.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "T84Xm_07qLlQ",
    "outputId": "a85b0f33-db4c-467b-c196-2a43aab04f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's nice to meet you.\n",
      "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"1. **Be clear and concise.** AI can't understand vague or ambiguous language.\\n2. **Use specific examples.** This will help the AI to understand what you're talking about.\\n3. **Be patient.** AI is still under development and it may take some time for it to understand you.\\n4. **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\\n\\nHere are some tips for having a conversation with an AI:\\n\\n* **Start with a greeting.** This will help to establish a rapport with the AI.\\n* **Ask open-ended questions.** This will give the AI more opportunities to respond.\\n* **Follow up on the AI's responses.** This will show that you're interested in what it has to say.\\n* **Be patient.** AI is still under development and it may take some time for it to understand you.\\n* **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Nothing much! Just tell me how do a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "0ckLlsGiqdSH",
    "outputId": "35d77981-8eb9-40a4-fbbf-7c3bb3b42943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's nice to meet you.\n",
      "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
      "AI: 1. **Be clear and concise.** AI can't understand vague or ambiguous language.\n",
      "2. **Use specific examples.** This will help the AI to understand what you're talking about.\n",
      "3. **Be patient.** AI is still under development and it may take some time for it to understand you.\n",
      "4. **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\n",
      "\n",
      "Here are some tips for having a conversation with an AI:\n",
      "\n",
      "* **Start with a greeting.** This will help to establish a rapport with the AI.\n",
      "* **Ask open-ended questions.** This will give the AI more opportunities to respond.\n",
      "* **Follow up on the AI's responses.** This will show that you're interested in what it has to say.\n",
      "* **Be patient.** AI is still under development and it may take some time for it to understand you.\n",
      "* **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\n",
      "Human: how many tips are there can you mention in the numbers\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'There are 5 tips mentioned in the numbers:\\n\\n1. Be clear and concise.\\n2. Use specific examples.\\n3. Be patient.\\n4. Have fun!\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"how many tips are there can you mention in the numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "by5vIxH2CS3Z",
    "outputId": "57a704bd-dcb1-44ca-87e7-b47329006f16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi there!'),\n",
       " AIMessage(content=\"Hello! It's nice to meet you.\"),\n",
       " HumanMessage(content='Nothing much! Just tell me how do a conversation with an AI.'),\n",
       " AIMessage(content=\"1. **Be clear and concise.** AI can't understand vague or ambiguous language.\\n2. **Use specific examples.** This will help the AI to understand what you're talking about.\\n3. **Be patient.** AI is still under development and it may take some time for it to understand you.\\n4. **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\\n\\nHere are some tips for having a conversation with an AI:\\n\\n* **Start with a greeting.** This will help to establish a rapport with the AI.\\n* **Ask open-ended questions.** This will give the AI more opportunities to respond.\\n* **Follow up on the AI's responses.** This will show that you're interested in what it has to say.\\n* **Be patient.** AI is still under development and it may take some time for it to understand you.\\n* **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\"),\n",
       " HumanMessage(content='how many tips are there can you mention in the numbers'),\n",
       " AIMessage(content='There are 5 tips mentioned in the numbers:\\n\\n1. Be clear and concise.\\n2. Use specific examples.\\n3. Be patient.\\n4. Have fun!\\n\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "rmlGoT6IQsKV",
    "outputId": "5f27894a-2875-4864-cab2-6b434bad8152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's nice to meet you.\n",
      "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
      "AI: 1. **Be clear and concise.** AI can't understand vague or ambiguous language.\n",
      "2. **Use specific examples.** This will help the AI to understand what you're talking about.\n",
      "3. **Be patient.** AI is still under development and it may take some time for it to understand you.\n",
      "4. **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\n",
      "\n",
      "Here are some tips for having a conversation with an AI:\n",
      "\n",
      "* **Start with a greeting.** This will help to establish a rapport with the AI.\n",
      "* **Ask open-ended questions.** This will give the AI more opportunities to respond.\n",
      "* **Follow up on the AI's responses.** This will show that you're interested in what it has to say.\n",
      "* **Be patient.** AI is still under development and it may take some time for it to understand you.\n",
      "* **Have fun!** Conversations with AI can be a lot of fun. Just relax and be yourself.\n",
      "Human: how many tips are there can you mention in the numbers\n",
      "AI: There are 5 tips mentioned in the numbers:\n",
      "\n",
      "1. Be clear and concise.\n",
      "2. Use specific examples.\n",
      "3. Be patient.\n",
      "4. Have fun!\n",
      "\n",
      "\n",
      "Human: can you give me the 3rd tip?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The 3rd tip is: **Be patient.**'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"can you give me the 3rd tip?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayGsHEv9qqNa"
   },
   "source": [
    "# **Conversation Buffer Window Memory**\n",
    "Conversation Buffer Window Memory is a variant of conversation memory in LangChain that focuses on retaining only the most recent interactions within a defined \"window\" size. This is particularly useful for managing token limits when working with large language models, as it avoids the overhead of including the entire conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "M5MfA3coql_o"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WADVyAaIq12p"
   },
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "afJtNTWrq9Z_"
   },
   "outputs": [],
   "source": [
    "window_memory.save_context(\n",
    "    {\"input\": \"Hi\"},\n",
    "    {\"output\": \"What's up\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ymZ7WpfYrR7_"
   },
   "outputs": [],
   "source": [
    "window_memory.save_context(\n",
    "    {\"input\": \"Not much, just hanging\"},\n",
    "    {\"output\": \"Cool\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUmMWst6rZdH",
    "outputId": "50b89e06-ec98-494b-bddc-00272b452b22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "92JAkps_rcHo"
   },
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory( k=2, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4I5696VUr4H6"
   },
   "outputs": [],
   "source": [
    "window_memory.save_context(\n",
    "    {\"input\": \"Hi\"},\n",
    "    {\"output\": \"What's up\"}\n",
    ")\n",
    "window_memory.save_context(\n",
    "    {\"input\": \"Not much, just hanging\"},\n",
    "    {\"output\": \"Cool\"}\n",
    ")\n",
    "window_memory.save_context(\n",
    "    {\"input\": \"ok thanks \"},\n",
    "    {\"output\": \"great thankyou\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTz6A1ZUr-3Q",
    "outputId": "a21e9c92-a192-46bf-971f-1693bd33ba5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Not much, just hanging'),\n",
       "  AIMessage(content='Cool'),\n",
       "  HumanMessage(content='ok thanks '),\n",
       "  AIMessage(content='great thankyou')]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gSLZAyaCsBSw"
   },
   "outputs": [],
   "source": [
    "conversation_window = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "c57Fr0kasY3g",
    "outputId": "7071ffae-b81c-4e18-d73f-793d65068188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"Hi, what's up?\"), AIMessage(content='Not much, just hanging.'), HumanMessage(content='how we can talk with AI'), AIMessage(content='There are many ways to communicate with AI. One way is through natural language processing (NLP), which allows AI to understand and respond to human language. Another way is through machine learning, which allows AI to learn from data and improve its performance over time.')]\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Not much, just hanging.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_window.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "Cb_2zJAgshtv",
    "outputId": "60b35011-526d-4dde-8aa1-243df0640938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='how we can talk with AI'), AIMessage(content='There are many ways to communicate with AI. One way is through natural language processing (NLP), which allows AI to understand and respond to human language. Another way is through machine learning, which allows AI to learn from data and improve its performance over time.'), HumanMessage(content=\"Hi, what's up?\"), AIMessage(content='Not much, just hanging.')]\n",
      "Human: how we can talk with AI give me 5 points\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1. **Natural language processing (NLP)**: This allows AI to understand and respond to human language.\\n2. **Machine learning**: This allows AI to learn from data and improve its performance over time.\\n3. **Speech recognition**: This allows AI to convert spoken words into text.\\n4. **Computer vision**: This allows AI to \"see\" and interpret images.\\n5. **Robotics**: This allows AI to control physical devices, such as robots.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_window.predict(input=\"how we can talk with AI give me 5 points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "cqjuwol7soep",
    "outputId": "d3bfead1-8854-4e21-b787-7b4e4ecfff14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='can you tell me how many tips you genearte in the previous to previous message?'), AIMessage(content='5'), HumanMessage(content='what was the fifth number tips?'), AIMessage(content='I do not have that information in context.')]\n",
      "Human: what is a allows AI to 'see' and 'interpret' images?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Convolutional Neural Network'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_window.predict(input=\"what is a allows AI to 'see' and 'interpret' images?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "v5bzPXTys_mw",
    "outputId": "a0ac905b-2e4f-4851-f425-18238ff600d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='what was the fifth number tips?'), AIMessage(content='I do not have that information in context.'), HumanMessage(content=\"what is a allows AI to 'see' and 'interpret' images?\"), AIMessage(content='Convolutional Neural Network')]\n",
      "Human: can you tell me how many tips you genearte in the previous to previous message?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I do not have that information in context.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_window.predict(input=\"can you tell me how many tips you genearte in the previous to previous message?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "Dq4X_PVYtJnx",
    "outputId": "23ac5f3c-c779-464a-ad0a-4f0b248c446b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"what is a allows AI to 'see' and 'interpret' images?\"), AIMessage(content='Computer vision'), HumanMessage(content='can you tell me how many tips you genearte in the previous to previous message?'), AIMessage(content='5')]\n",
      "Human: what was the fifth number tips?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I do not have that information in context.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what was the fifth number tips?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ConversationEntityMemory**\n",
    "\n",
    "ConversationEntityMemory in LangChain is a specialized memory module that tracks information about specific entities mentioned during a conversation. This type of memory allows the conversational agent to store, retrieve, and recall details about entities dynamically, enabling more personalized and context-aware interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "11Sj-AcdvN-S"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "gx7UfR2b55Wy"
   },
   "outputs": [],
   "source": [
    "memory = ConversationEntityMemory(llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "h7H3aQ5t6Esy"
   },
   "outputs": [],
   "source": [
    "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prEkB6V16IHr",
    "outputId": "a5b093a9-b51e-4469-e212-f873dd0650eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '', 'entities': {'Deven': '', 'Sam': ''}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "iEJTouF96LH7"
   },
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaefDOzq6mmS",
    "outputId": "395b1892-ca17-424a-9af4-c1dc1ca9ca6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?',\n",
       " 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Sam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "9eHY8f8A7E0K"
   },
   "outputs": [],
   "source": [
    "memory = ConversationEntityMemory(llm=model, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKCuHE6K8Dvy",
    "outputId": "fb4bd10c-c3ea-4c5e-9406-2a9ceb9e02ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationEntityMemory(return_messages=True, llm=ChatGoogleGenerativeAI(model='models/gemini-1.0-pro', client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7bf6bc68d1e0>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7bf6bc4aed40>, default_metadata=(), convert_system_message_to_human=True))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "PzBxI_6J8FCc"
   },
   "outputs": [],
   "source": [
    "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZj9GaBH8Hej",
    "outputId": "e877af0f-cee9-4304-8a49-bb4a47510480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [], 'entities': {'Deven': '', 'Sam': ''}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "r0Sa4O9C8Otz"
   },
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLBPaeWU8SUL",
    "outputId": "9ac3bf92-ff52-48b1-d3ad-20c0c249a9d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Deven & Sam are working on a hackathon project'),\n",
       "  AIMessage(content=' That sounds like a great project! What kind of project are they working on?')],\n",
       " 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Sam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "25sJTv-J8cRb"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "tcLlxQe4_MBI"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "UybNNJCA_N2X"
   },
   "outputs": [],
   "source": [
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "g9-ALiFc_SQ8"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "7kQLI2Tk_U2C"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "KGqFtiST_Wf6"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "I1i6FFEE_u_D",
    "outputId": "79197e71-b89e-467f-b0d1-5be7e015bba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Deven': '', 'Sam': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"That's great! What kind of project are they working on?\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Deven & Sam are working on a hackathon project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDdJUJjd_4Ol",
    "outputId": "1a1c9bd1-e334-45cb-d370-d6bd1d08f948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is working on a hackathon project with Sam.',\n",
       " 'Sam': 'Sam is working on a hackathon project with Deven.'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "7_3ELrYJAB57",
    "outputId": "4c0b9050-b80e-4682-c4d0-063cda7cee79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Langchain': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "AI: That's great! What kind of project are they working on?\n",
      "Last line:\n",
      "Human: They are trying to add more complex memory structures to Langchain\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"That's an interesting project! Langchain is a very powerful language model, and adding more complex memory structures could significantly enhance its capabilities. What specific types of memory structures are they considering adding?\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are trying to add more complex memory structures to Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "FAToSArZAGJy",
    "outputId": "f458234e-cb17-4706-82d7-db05aee12afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{}\n",
      "\n",
      "Current conversation:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "AI: That's great! What kind of project are they working on?\n",
      "Human: They are trying to add more complex memory structures to Langchain\n",
      "AI: That's an interesting project! Langchain is a very powerful language model, and adding more complex memory structures could significantly enhance its capabilities. What specific types of memory structures are they considering adding?\n",
      "Last line:\n",
      "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"That's a great idea! A key-value store would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation. How are they planning to integrate the key-value store with the existing Langchain architecture?\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are adding in a key-value store for entities mentioned so far in the conversation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "JMqcPvC3AJEC",
    "outputId": "b3991c52-de54-4436-abfe-1b9640203e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Deven': 'Deven is working on a hackathon project with Sam.', 'Sam': 'Sam is working on a hackathon project with Deven.'}\n",
      "\n",
      "Current conversation:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "AI: That's great! What kind of project are they working on?\n",
      "Human: They are trying to add more complex memory structures to Langchain\n",
      "AI: That's an interesting project! Langchain is a very powerful language model, and adding more complex memory structures could significantly enhance its capabilities. What specific types of memory structures are they considering adding?\n",
      "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
      "AI: That's a great idea! A key-value store would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation. How are they planning to integrate the key-value store with the existing Langchain architecture?\n",
      "Last line:\n",
      "Human: What do you know about Deven & Sam?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I know that Deven and Sam are working on a hackathon project together. They are trying to add more complex memory structures to Langchain, a powerful language model. Specifically, they are adding in a key-value store for entities mentioned so far in the conversation. I think this is a great idea, as it would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What do you know about Deven & Sam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJkuNNOZAKri",
    "outputId": "9c448519-a611-49e0-c50e-79ce374ce544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deven': 'Deven is working on a hackathon project with Sam, where they are '\n",
      "          'trying to add more complex memory structures to Langchain, '\n",
      "          'specifically a key-value store for entities mentioned so far in the '\n",
      "          'conversation.',\n",
      " 'Langchain': 'Langchain is a powerful language model that Deven and Sam are '\n",
      "              'working on enhancing by adding more complex memory structures.',\n",
      " 'Sam': 'Sam is working on a hackathon project with Deven, specifically adding '\n",
      "        'a key-value store to Langchain.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "sI7tSPB7AN8b",
    "outputId": "6a7660a5-c92a-4488-a25e-97bfe401086f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Daimon': '', 'Sam': 'Sam is working on a hackathon project with Deven, specifically adding a key-value store to Langchain.'}\n",
      "\n",
      "Current conversation:\n",
      "Human: They are trying to add more complex memory structures to Langchain\n",
      "AI: That's an interesting project! Langchain is a very powerful language model, and adding more complex memory structures could significantly enhance its capabilities. What specific types of memory structures are they considering adding?\n",
      "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
      "AI: That's a great idea! A key-value store would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation. How are they planning to integrate the key-value store with the existing Langchain architecture?\n",
      "Human: What do you know about Deven & Sam?\n",
      "AI: I know that Deven and Sam are working on a hackathon project together. They are trying to add more complex memory structures to Langchain, a powerful language model. Specifically, they are adding in a key-value store for entities mentioned so far in the conversation. I think this is a great idea, as it would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation.\n",
      "Last line:\n",
      "Human: Sam is the founder of a company called Daimon.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"That's interesting! I'm not familiar with Daimon, but I'm always interested in hearing about new companies. What does Daimon do?\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Sam is the founder of a company called Daimon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Mqxyr1mAQyk",
    "outputId": "9878d6ab-8e25-4092-fe9b-a6c95c6def64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Daimon': 'Daimon is a company founded by Sam.',\n",
      " 'Deven': 'Deven is working on a hackathon project with Sam, where they are '\n",
      "          'trying to add more complex memory structures to Langchain, '\n",
      "          'specifically a key-value store for entities mentioned so far in the '\n",
      "          'conversation.',\n",
      " 'Langchain': 'Langchain is a powerful language model that Deven and Sam are '\n",
      "              'working on enhancing by adding more complex memory structures.',\n",
      " 'Sam': 'Sam is working on a hackathon project with Deven, specifically adding '\n",
      "        'a key-value store to Langchain. Sam is also the founder of a company '\n",
      "        'called Daimon.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "yKVKMXI_ATYk",
    "outputId": "22bbbbd9-92bd-4ac6-e8a3-61d4b3cb2e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Sam': 'Sam is working on a hackathon project with Deven, specifically adding a key-value store to Langchain. Sam is also the founder of a company called Daimon.'}\n",
      "\n",
      "Current conversation:\n",
      "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
      "AI: That's a great idea! A key-value store would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation. How are they planning to integrate the key-value store with the existing Langchain architecture?\n",
      "Human: What do you know about Deven & Sam?\n",
      "AI: I know that Deven and Sam are working on a hackathon project together. They are trying to add more complex memory structures to Langchain, a powerful language model. Specifically, they are adding in a key-value store for entities mentioned so far in the conversation. I think this is a great idea, as it would allow Langchain to keep track of important entities and their attributes, which could be very useful for tasks such as question answering and dialogue generation.\n",
      "Human: Sam is the founder of a company called Daimon.\n",
      "AI: That's interesting! I'm not familiar with Daimon, but I'm always interested in hearing about new companies. What does Daimon do?\n",
      "Last line:\n",
      "Human: What do you know about Sam?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I know that Sam is working on a hackathon project with Deven, specifically adding a key-value store to Langchain. I also know that Sam is the founder of a company called Daimon.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What do you know about Sam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XfVuH3CAWGj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMAIeL7BnRH5Lu0OSgsgW4c",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
